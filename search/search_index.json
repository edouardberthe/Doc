{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs \u00b6 For full documentation visit mkdocs.org . Commands \u00b6 mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout \u00b6 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Welcome to MkDocs"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"code-review/","text":"Best Practices for Peer Code Review \u00b6 Original article from Cisco : BestPracticesForPeerCodeReview.pdf Summary of the article: \u00b6 \"Review fewer than 200-400 lines of code at a time within 60-90min, 70-90% defect\" --> Don't review too much at the same time \"Aim for an inspection rate of less than 300-500 LOC/hour** (6-8 LOC/mn!)\" --> Don't review too quickly \"Take enough time for a proper, slow review, but not more than 60-90 minutes\" --> Don't spend too much time Authors should annotate source code before the review begins --> Describe your dev Authors might be able to eliminate most defects themselves. They should \"prepare\" the CR by annotating code. These comments are not code comments but comments for reviewers . By doing so, they find themselves potential bugs in their devs. Potential explanation of why fewer bugs: where there are comments, the reviewers is \"biased\" and see less bugs (because his spirit has been anchored to the developer interpretation). But after investigation, this is not the case. Establish quantifiable goals for code review and capture metrics so you can improve your processes --> Measure the impact of the CR Checklists substantially improve results for both authors and reviewers Verify that defects are actually fixed! Managers must foster a good code review culture in which finding defects is viewed positively Easy to see defects negatively (mistakes in the code), but it should be seen as a positive mean for learning, growing and communication . It's not an accusation way of working: the author mades the bugs and the reviewer saw it, but more as en efficient pair-programming session. To maintain a consistent message that finding bugs is good, management must promise that defect densities will never be used in performance reports Beware the \u201cBig Brother\u201d effect The Ego Effect: Do at least some code review, even if you don\u2019t have time to review it all The Ego Effect: knowing that your code will be reviewed makes you more conscientious before pushing your work. This effect is also the case even if the review is not compulsory: reviewing 20-30% of the code would probably be optimal to assure the Ego Effect with minimal time expenditure. Lightweight-style code reviews are efficient, practical, and effective at finding bugs --> GitHub is pretty nice!","title":"Code Review"},{"location":"code-review/#best-practices-for-peer-code-review","text":"Original article from Cisco : BestPracticesForPeerCodeReview.pdf","title":"Best Practices for Peer Code Review"},{"location":"code-review/#summary-of-the-article","text":"\"Review fewer than 200-400 lines of code at a time within 60-90min, 70-90% defect\" --> Don't review too much at the same time \"Aim for an inspection rate of less than 300-500 LOC/hour** (6-8 LOC/mn!)\" --> Don't review too quickly \"Take enough time for a proper, slow review, but not more than 60-90 minutes\" --> Don't spend too much time Authors should annotate source code before the review begins --> Describe your dev Authors might be able to eliminate most defects themselves. They should \"prepare\" the CR by annotating code. These comments are not code comments but comments for reviewers . By doing so, they find themselves potential bugs in their devs. Potential explanation of why fewer bugs: where there are comments, the reviewers is \"biased\" and see less bugs (because his spirit has been anchored to the developer interpretation). But after investigation, this is not the case. Establish quantifiable goals for code review and capture metrics so you can improve your processes --> Measure the impact of the CR Checklists substantially improve results for both authors and reviewers Verify that defects are actually fixed! Managers must foster a good code review culture in which finding defects is viewed positively Easy to see defects negatively (mistakes in the code), but it should be seen as a positive mean for learning, growing and communication . It's not an accusation way of working: the author mades the bugs and the reviewer saw it, but more as en efficient pair-programming session. To maintain a consistent message that finding bugs is good, management must promise that defect densities will never be used in performance reports Beware the \u201cBig Brother\u201d effect The Ego Effect: Do at least some code review, even if you don\u2019t have time to review it all The Ego Effect: knowing that your code will be reviewed makes you more conscientious before pushing your work. This effect is also the case even if the review is not compulsory: reviewing 20-30% of the code would probably be optimal to assure the Ego Effect with minimal time expenditure. Lightweight-style code reviews are efficient, practical, and effective at finding bugs --> GitHub is pretty nice!","title":"Summary of the article:"},{"location":"cryptography/","text":"PGP, OpenPGP, GPG \u00b6 PGP: \"Pretty Good Privacy\" \u00b6 Private software, originally created by PGP Inc. Use several methods to allow encryption of data (hash, data compression, symmetric cryptography, public key cryptography) OpenPGP \u00b6 Phil Zimmerman, which worked at PGP Inc., wanted to security and cryptography for privacy to be free. So he created OpenPGP, an open source norm for PGP. GPG: GNU Privacy Guard \u00b6 GPG is the implementation by GNU.","title":"Cryptography"},{"location":"cryptography/#pgp-openpgp-gpg","text":"","title":"PGP, OpenPGP, GPG"},{"location":"cryptography/#pgp-pretty-good-privacy","text":"Private software, originally created by PGP Inc. Use several methods to allow encryption of data (hash, data compression, symmetric cryptography, public key cryptography)","title":"PGP: \"Pretty Good Privacy\""},{"location":"cryptography/#openpgp","text":"Phil Zimmerman, which worked at PGP Inc., wanted to security and cryptography for privacy to be free. So he created OpenPGP, an open source norm for PGP.","title":"OpenPGP"},{"location":"cryptography/#gpg-gnu-privacy-guard","text":"GPG is the implementation by GNU.","title":"GPG: GNU Privacy Guard"},{"location":"docker/","text":"Update all images \u00b6 di --format \"{{.Repository}}:{{.Tag}}\" | xargs -L1 docker pull","title":"Docker"},{"location":"docker/#update-all-images","text":"di --format \"{{.Repository}}:{{.Tag}}\" | xargs -L1 docker pull","title":"Update all images"},{"location":"efcore/","text":"Entity Framework Core \u00b6 Entity Framework Core is an ORM (Object Relational Mapper), which is a library helping developer to handle links to database. It handles automatic mapping between database tables (relational) and \"Plain Old CLR Objects\" (POCO, POJO in Java, etc.). Entity Framework Core in Action \u00b6 Book written by Jon Smith Chapter 2: Querying database \u00b6 Objects to create: Entity Models : C# objects mapped to the database scheme, which will represent tables DBContext : configuring database (connection string, perf, etc.) + declaring the Entity Models DTO objects (for \"Data Transfering Object\"): objects aiming at containing data for transferring it between 2 parts of the library a function IQueryable<BookDto> MapBookToDto(IQueryable<Book>) which will transform the SQL Query into a DTO object for presentation sorting, filtering, paging functions: IQueryable<BookDto> Filtering(IQueryable<BookDto>, FilteringOptions) Layered Architecture: for small-to-medium sized projects: a DataLayer: DBContext + Entity Models. Has no knowledge of any other layer a ServiceLayer: implementing the MapBookToDto function + filtering, sorting, paging functions a PresentationLayer (e.g. ASP.Net Core): calling the functions defined in the ServiceLayer and send the DTO objects to the views (e.g. HTML pages)","title":"Entity Framework"},{"location":"efcore/#entity-framework-core","text":"Entity Framework Core is an ORM (Object Relational Mapper), which is a library helping developer to handle links to database. It handles automatic mapping between database tables (relational) and \"Plain Old CLR Objects\" (POCO, POJO in Java, etc.).","title":"Entity Framework Core"},{"location":"efcore/#entity-framework-core-in-action","text":"Book written by Jon Smith","title":"Entity Framework Core in Action"},{"location":"efcore/#chapter-2-querying-database","text":"Objects to create: Entity Models : C# objects mapped to the database scheme, which will represent tables DBContext : configuring database (connection string, perf, etc.) + declaring the Entity Models DTO objects (for \"Data Transfering Object\"): objects aiming at containing data for transferring it between 2 parts of the library a function IQueryable<BookDto> MapBookToDto(IQueryable<Book>) which will transform the SQL Query into a DTO object for presentation sorting, filtering, paging functions: IQueryable<BookDto> Filtering(IQueryable<BookDto>, FilteringOptions) Layered Architecture: for small-to-medium sized projects: a DataLayer: DBContext + Entity Models. Has no knowledge of any other layer a ServiceLayer: implementing the MapBookToDto function + filtering, sorting, paging functions a PresentationLayer (e.g. ASP.Net Core): calling the functions defined in the ServiceLayer and send the DTO objects to the views (e.g. HTML pages)","title":"Chapter 2: Querying database"},{"location":"kubernetes/","text":"Basics \u00b6 Notions: Pod: smallest unit in Kubernetes. A pod can be a set of several containers. For instance an app Node.js + a database PostgreSQL with it. Node: a Node is a worker machine , it can be a physical or virtual machine, on which there will be several Pods. A Node has at least: a Docker runtime (to pull images, run the containers inside the pods, etc.) a Kubelet, which is the process allowing to communicate with the Master Node Deployment: Service: abstraction which defines a logical set of Pods and a policy by which to access them. Pods can die, and can be replaced, but we need something to store the identification of each one (for instance IPs) Main commands \u00b6 kubectl get : list resources kubectl describe : show detailed information about a resource kubectl logs : print the logs from a container in a pod kubectl exec : execute a command on a container in a pod Pods are mortal . They have a lifecycle. Minikube \u00b6 Minikube is a tool used to test Kubernetes. It creates a Kubernetes cluster with only one Nod, inside a Virtual Machine. Some useful commands are available with it: minikube service <service-name> minikube dashboard --> extremely useful. Open a Dashboard to have an overlook of the cluster (only one node because of Minikube but still cool). Minikube with hyperkit How to interact with Kubernetes \u00b6 They are three ways to interact with Kubernetes: Imperative commmands Imperative object configuration Declarative object configuration","title":"Kubernetes"},{"location":"kubernetes/#basics","text":"Notions: Pod: smallest unit in Kubernetes. A pod can be a set of several containers. For instance an app Node.js + a database PostgreSQL with it. Node: a Node is a worker machine , it can be a physical or virtual machine, on which there will be several Pods. A Node has at least: a Docker runtime (to pull images, run the containers inside the pods, etc.) a Kubelet, which is the process allowing to communicate with the Master Node Deployment: Service: abstraction which defines a logical set of Pods and a policy by which to access them. Pods can die, and can be replaced, but we need something to store the identification of each one (for instance IPs)","title":"Basics"},{"location":"kubernetes/#main-commands","text":"kubectl get : list resources kubectl describe : show detailed information about a resource kubectl logs : print the logs from a container in a pod kubectl exec : execute a command on a container in a pod Pods are mortal . They have a lifecycle.","title":"Main commands"},{"location":"kubernetes/#minikube","text":"Minikube is a tool used to test Kubernetes. It creates a Kubernetes cluster with only one Nod, inside a Virtual Machine. Some useful commands are available with it: minikube service <service-name> minikube dashboard --> extremely useful. Open a Dashboard to have an overlook of the cluster (only one node because of Minikube but still cool). Minikube with hyperkit","title":"Minikube"},{"location":"kubernetes/#how-to-interact-with-kubernetes","text":"They are three ways to interact with Kubernetes: Imperative commmands Imperative object configuration Declarative object configuration","title":"How to interact with Kubernetes"},{"location":"linux/","text":"Monitoring temperature on Linux \u00b6 Monitoring CPU and GPU Temperatures on Linux From a Unix & Linux Stack Exchange question : Nagios: seems to be the best choice (standard) Munin Installation (Digital Ocean) Cacti Monitorix Solve (eternal) locales problems \u00b6 To solve locales problems (LC_ALL, LC_CTYPE, ...), simply comment the following line in /etc/ssh/sshd_config : AcceptEnv LC_* SSH Agent Forwarding \u00b6 From a user to root (doing sudo) \u00b6 The goal is to tell sudo to transfer the environment variable SSH_AUTH_SOCK to root. For that, open the /etc/sudoers file [StackOverflow source]","title":"Linux"},{"location":"linux/#monitoring-temperature-on-linux","text":"Monitoring CPU and GPU Temperatures on Linux From a Unix & Linux Stack Exchange question : Nagios: seems to be the best choice (standard) Munin Installation (Digital Ocean) Cacti Monitorix","title":"Monitoring temperature on Linux"},{"location":"linux/#solve-eternal-locales-problems","text":"To solve locales problems (LC_ALL, LC_CTYPE, ...), simply comment the following line in /etc/ssh/sshd_config : AcceptEnv LC_*","title":"Solve (eternal) locales problems"},{"location":"linux/#ssh-agent-forwarding","text":"","title":"SSH Agent Forwarding"},{"location":"linux/#from-a-user-to-root-doing-sudo","text":"The goal is to tell sudo to transfer the environment variable SSH_AUTH_SOCK to root. For that, open the /etc/sudoers file [StackOverflow source]","title":"From a user to root (doing sudo)"},{"location":"physics/","text":"Einstein \u00b6 De l'\u00e9lectrodynamique des corps en mouvement","title":"Physics"},{"location":"physics/#einstein","text":"De l'\u00e9lectrodynamique des corps en mouvement","title":"Einstein"},{"location":"python/async/","text":"Concurrent development with Python \u00b6 Python allows concurrent programming thanks to the built-in library asyncio . Since Python 3.7: asyncio.run \u00b6 asyncio.run is the preferred (simplest) way to launch an asynchronous program (it should be launched from the highest-end point, like the main.py probably). The best way to understand how asyncio.run interacts with its counterparts ( EventLoop.run_until_complete , run_forever , etc.) is to see what is does. From this gist it can be seen that asyncio.run does the following: create a new event loop set the newly-created event loop as the current event loop run the coroutine util completed set the current event loop as None In Python code, it gives: new_loop = asyncio.new_event_loop() asyncio.set_event_loop(new_loop) new_loop.run_until_complete(coro) asyncio.set_event_loop(None) loop.close()","title":"Concurrent programming"},{"location":"python/async/#concurrent-development-with-python","text":"Python allows concurrent programming thanks to the built-in library asyncio .","title":"Concurrent development with Python"},{"location":"python/async/#since-python-37-asynciorun","text":"asyncio.run is the preferred (simplest) way to launch an asynchronous program (it should be launched from the highest-end point, like the main.py probably). The best way to understand how asyncio.run interacts with its counterparts ( EventLoop.run_until_complete , run_forever , etc.) is to see what is does. From this gist it can be seen that asyncio.run does the following: create a new event loop set the newly-created event loop as the current event loop run the coroutine util completed set the current event loop as None In Python code, it gives: new_loop = asyncio.new_event_loop() asyncio.set_event_loop(new_loop) new_loop.run_until_complete(coro) asyncio.set_event_loop(None) loop.close()","title":"Since Python 3.7: asyncio.run"},{"location":"python/gil/","text":"GIL in Python \u00b6 Resources: Blog article on realpython.org Talk by Larry Hastings during PyCon 2015: Python's Infamous GIL (referenced by the former) Summary: There is no Garbage Collector in Python, so we need to count the references of any object, so that when this count reaches 0 we can free the memory allocated to store this object. Problem is: when several threads involved, they can all have different references to the same object --> difficult to keep track of the references Solution taken for Python: simply disallowing it! There is a lock on the Python interpreter itself . If there are several threads involved, and if they want to execute native Python code, they have to wait for their turn to execute the Python interpreter. Dates: 1990: Python invented 1992: GIL added. At this time, there was no multi-core, because if you wanted to do it, you really had to physically handle 2 processors on the motherboard (so have a motherboard specially adapted for that). But now, the world is fully multi-core, since we managed to have several physical cores on the same processor Guido is aware of that, but he said he is ok to remove the GIL, only if it doesn't decrease performance on single-threaded examples --> very difficult to reach! There have been attempts to do multi-threading on Python in Python 1.4 without any API change. They took all the global variables and turn them into struct s (structures). The problem was of course of the references counting (as explained before). To solve this problem, they tried to create a resource which was keeping track of the counts of all references, but they had obviously to add a lock on this resource! To sum up: in order to remove the lock on the Python interpreter , they had to add a lock on the references counter , so that this resource could count the references efficiently in spite of the different threads running the Python interpreter simultaneously. But all the threads were constantly grabbing and releasing the lock so much, that the lock itself was causing a big performance issue --> between 4x and 7x slower! In order to manage the memory, another possibility than the GIL (having a counter keeping track of the number of references there are of all the objects) would be to have a \"Pure\" Garbage Collector. It works the following way: when the memory is full (at least, almost full, because if we wait to have the absolute fullness of the memory, therefore it must certainly be that we cannot do any further computation to release it, given that any computation would require at least some memory..), then we stop all the program computation, and we do an exploration of all the objects currently used in the Program. All the objects which cannot be \"reached\" by the exploration cannot be used, and can be \"collected\" (a nice word to say destroyed and removed). CPython is not the only implementation of Python interpretation. There are four main ones, which does not all have a GIL, but have a GC: Pure GC GIL cpython (C) no yes jython (java) yes no ironpython (C#) yes no pypy yes no* It's perfectly possible to have a working Python interpreter without any sort of GIL . Would it be possible to have GC instead of reference counting? Yes. Would it be as fast as reference counting? We can't know until we do it. Conventional wisdom about GC is that it is about the same speed as reference counting. But the real problem is that it would break all the C extensions API, and we can't afford that (Python 3 did that, and it seems that it is slowly adapted.. not sure though).","title":"GIL"},{"location":"python/gil/#gil-in-python","text":"Resources: Blog article on realpython.org Talk by Larry Hastings during PyCon 2015: Python's Infamous GIL (referenced by the former) Summary: There is no Garbage Collector in Python, so we need to count the references of any object, so that when this count reaches 0 we can free the memory allocated to store this object. Problem is: when several threads involved, they can all have different references to the same object --> difficult to keep track of the references Solution taken for Python: simply disallowing it! There is a lock on the Python interpreter itself . If there are several threads involved, and if they want to execute native Python code, they have to wait for their turn to execute the Python interpreter. Dates: 1990: Python invented 1992: GIL added. At this time, there was no multi-core, because if you wanted to do it, you really had to physically handle 2 processors on the motherboard (so have a motherboard specially adapted for that). But now, the world is fully multi-core, since we managed to have several physical cores on the same processor Guido is aware of that, but he said he is ok to remove the GIL, only if it doesn't decrease performance on single-threaded examples --> very difficult to reach! There have been attempts to do multi-threading on Python in Python 1.4 without any API change. They took all the global variables and turn them into struct s (structures). The problem was of course of the references counting (as explained before). To solve this problem, they tried to create a resource which was keeping track of the counts of all references, but they had obviously to add a lock on this resource! To sum up: in order to remove the lock on the Python interpreter , they had to add a lock on the references counter , so that this resource could count the references efficiently in spite of the different threads running the Python interpreter simultaneously. But all the threads were constantly grabbing and releasing the lock so much, that the lock itself was causing a big performance issue --> between 4x and 7x slower! In order to manage the memory, another possibility than the GIL (having a counter keeping track of the number of references there are of all the objects) would be to have a \"Pure\" Garbage Collector. It works the following way: when the memory is full (at least, almost full, because if we wait to have the absolute fullness of the memory, therefore it must certainly be that we cannot do any further computation to release it, given that any computation would require at least some memory..), then we stop all the program computation, and we do an exploration of all the objects currently used in the Program. All the objects which cannot be \"reached\" by the exploration cannot be used, and can be \"collected\" (a nice word to say destroyed and removed). CPython is not the only implementation of Python interpretation. There are four main ones, which does not all have a GIL, but have a GC: Pure GC GIL cpython (C) no yes jython (java) yes no ironpython (C#) yes no pypy yes no* It's perfectly possible to have a working Python interpreter without any sort of GIL . Would it be possible to have GC instead of reference counting? Yes. Would it be as fast as reference counting? We can't know until we do it. Conventional wisdom about GC is that it is about the same speed as reference counting. But the real problem is that it would break all the C extensions API, and we can't afford that (Python 3 did that, and it seems that it is slowly adapted.. not sure though).","title":"GIL in Python"},{"location":"python/packaging/","text":"Packaging \u00b6 \u00c9tat de l'art du packaging python","title":"Packaging"},{"location":"python/packaging/#packaging","text":"\u00c9tat de l'art du packaging python","title":"Packaging"},{"location":"python/pep/","text":"Important PEP to remember \u00b6 Generators and synchronicity \u00b6 PEP 342 -- Coroutines via Enhanced Generators (10 May 2005, Python 2.5) In particular, this solves: PEP 288 (2002): Generators Attributes and Exceptions (withdrawn) PEP 325 (25/08/2003): Resource-Release Support for Generators (rejected) PEP 380 -- Syntax for Delegating to a Subgenerator (13 Feb 2009, Python 3.3) Basically, implements the yield from operator, allowing to transfer the generation to another generator. If only yield as statement are used, this is equivalent to iterating over the subgenerator via a classical for loop, but if the yield is used as an expression (for instance in x = yield ), meaning that the generator is actually a coroutine, then the yield from expression also transfer: the data sent by the send() command the errors sent by the throw() command the exception sent by the close() command PEP 492 -- Coroutines with async and await syntax (Python 3.5) 09 Apr. 2015: Proposed 05 May 2015: Accepted by Guido 11 May 2015: Merged Type hints: \u00b6 PEP 484 -- Type Hints (Python 3.5), by Guido. The most important one. PEP 483 -- The Theory of Type Hints (Informational) Lays down the theory of the previous one PEP 526 -- Syntax for Variable Annotations (Python 3.6, 09/08/2016): Allow to annotate variables directly instead of inside comments. Before: a = \"coucou\" # type: str After: a: str = \"coucou\" PEP 585 -- Type Hinting Generics In Standard Collections (03/03/2019) Abstract: \"Static typing as defined by PEPs 484, 526, 544, 560, and 563 was built incrementally on top of the existing Python runtime and constrained by existing syntax and runtime behavior. This led to the existence of a duplicated collection hierarchy in the typing module due to generics (for example typing.List and the built-in list). This PEP proposes to enable support for the generics syntax in all standard collections currently available in the typing module.\"","title":"PEP"},{"location":"python/pep/#important-pep-to-remember","text":"","title":"Important PEP to remember"},{"location":"python/pep/#generators-and-synchronicity","text":"PEP 342 -- Coroutines via Enhanced Generators (10 May 2005, Python 2.5) In particular, this solves: PEP 288 (2002): Generators Attributes and Exceptions (withdrawn) PEP 325 (25/08/2003): Resource-Release Support for Generators (rejected) PEP 380 -- Syntax for Delegating to a Subgenerator (13 Feb 2009, Python 3.3) Basically, implements the yield from operator, allowing to transfer the generation to another generator. If only yield as statement are used, this is equivalent to iterating over the subgenerator via a classical for loop, but if the yield is used as an expression (for instance in x = yield ), meaning that the generator is actually a coroutine, then the yield from expression also transfer: the data sent by the send() command the errors sent by the throw() command the exception sent by the close() command PEP 492 -- Coroutines with async and await syntax (Python 3.5) 09 Apr. 2015: Proposed 05 May 2015: Accepted by Guido 11 May 2015: Merged","title":"Generators and synchronicity"},{"location":"python/pep/#type-hints","text":"PEP 484 -- Type Hints (Python 3.5), by Guido. The most important one. PEP 483 -- The Theory of Type Hints (Informational) Lays down the theory of the previous one PEP 526 -- Syntax for Variable Annotations (Python 3.6, 09/08/2016): Allow to annotate variables directly instead of inside comments. Before: a = \"coucou\" # type: str After: a: str = \"coucou\" PEP 585 -- Type Hinting Generics In Standard Collections (03/03/2019) Abstract: \"Static typing as defined by PEPs 484, 526, 544, 560, and 563 was built incrementally on top of the existing Python runtime and constrained by existing syntax and runtime behavior. This led to the existence of a duplicated collection hierarchy in the typing module due to generics (for example typing.List and the built-in list). This PEP proposes to enable support for the generics syntax in all standard collections currently available in the typing module.\"","title":"Type hints:"}]}